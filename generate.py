import os
import torch
import torchaudio
import numpy as np
import matplotlib.pyplot as plt
from matcha.models.matcha_tts import MatchaTTS
from matcha.text_to_ID.text_to_sequence import text_to_sequence

# --- CONFIGURATION ---
CHECKPOINT_PATH = None  # Laissera le script trouver le dernier automatiquement
OUTPUT_FOLDER = "generated_audio"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
TEXTE_A_DIRE = "Hello, this is a test of the Matcha TTS model."

def get_latest_checkpoint(logs_dir="lightning_logs"):
    """Trouve automatiquement le dernier fichier .ckpt pour avoir les derniers poids."""
    import glob
    # Cherche r√©cursivement tous les .ckpt
    files = glob.glob(f"{logs_dir}/**/*.ckpt", recursive=True)
    if not files:
        raise FileNotFoundError("Aucun checkpoint trouv√© ! As-tu lanc√© l'entra√Ænement ?")
    # Trie par date de modification (le plus r√©cent en dernier)
    latest_file = max(files, key=os.path.getmtime)
    print(f"‚úÖ Checkpoint trouv√© : {latest_file}")
    return latest_file

def simple_euler_ode_solver(model, mu, n_steps=10):
    """
    Le c≈ìur du Flow Matching : transforme le bruit en son pas √† pas.
    """
    # 1. On part d'un bruit blanc (t=0)
    # mu shape: [1, 80, T]
    z = torch.randn_like(mu, device=DEVICE)
    
    # 2. On avance dans le temps de 0 √† 1
    dt = 1.0 / n_steps
    
    print(f"üîÑ G√©n√©ration en {n_steps} √©tapes...")
    
    for i in range(n_steps):
        t_val = i / n_steps
        t = torch.tensor([t_val], device=DEVICE)
        
        # Le d√©codeur pr√©dit la vitesse (le vecteur direction)
        # On n'a pas besoin de masque ici car on g√©n√®re tout
        v_pred = model.decoder(z, t, mu, mask=None)
        
        # Euler step : nouvelle position = ancienne + vitesse * temps
        z = z + v_pred * dt
        
    return z # C'est notre spectrogramme g√©n√©r√© (y_hat)

def main():
    os.makedirs(OUTPUT_FOLDER, exist_ok=True)

    # 1. Chargement du mod√®le
    ckpt = CHECKPOINT_PATH if CHECKPOINT_PATH else get_latest_checkpoint()
    print("‚è≥ Chargement du mod√®le...")
    
    # On charge le mod√®le et ses hyperparam√®tres
    model = MatchaTTS.load_from_checkpoint(ckpt)
    model.to(DEVICE)
    model.eval() # Mode √©valuation (d√©sactive le dropout)

    # 2. Pr√©paration du texte
    print(f"üìñ Texte : '{TEXTE_A_DIRE}'")
    sequence = text_to_sequence(TEXTE_A_DIRE, ["english_cleaners"]) # Ou basic_cleaners
    x = torch.tensor([sequence], dtype=torch.long, device=DEVICE)
    x_lengths = torch.tensor([len(sequence)], dtype=torch.long, device=DEVICE)

    with torch.no_grad():
        # 3. Encodage (Texte -> Mu)
        mu, _ = model.encoder(x, x_lengths)
        
        # --- ALIGNEMENT MANUEL (IMPORTANT) ---
        # Comme on a utilis√© l'interpolation √† l'entra√Ænement, on doit le refaire ici.
        # On d√©cide arbitrairement qu'un caract√®re dure X temps.
        # Facteur 5 = chaque lettre dure un peu de temps. Ajuste si √ßa parle trop vite/lentement.
        duree_audio_cible = mu.shape[-1] * 5 
        mu = torch.nn.functional.interpolate(mu, size=duree_audio_cible, mode='nearest')
        
        # 4. Flow Matching (G√©n√©ration du Spectrogramme)
        spectrogram = simple_euler_ode_solver(model, mu, n_steps=50)

    # 5. Conversion Spectrogramme -> Audio (Griffin-Lim)
    # C'est une m√©thode math√©matique pour reconstruire le son sans Vocoder entra√Æn√©
    # 5. Conversion Spectrogramme -> Audio (Inverse Mel + Griffin-Lim)
    print("üîä Conversion en audio (InvMel -> Griffin-Lim)...")
    
    # A. Cr√©ation de la transformation Inverse Mel (Pour passer de 80 -> 513 canaux)
    # On doit utiliser les m√™mes param√®tres que ceux utilis√©s pour cr√©er le dataset LJSpeech
    inv_mel_scale = torchaudio.transforms.InverseMelScale(
        n_stft=1024 // 2 + 1,  # = 513 bins de fr√©quence
        n_mels=80,
        sample_rate=22050,
        f_min=0.0,
        f_max=8000.0,
        norm='slaney',
        mel_scale='slaney' 
    ).to(DEVICE)

    # B. Configuration de Griffin-Lim (Pour passer de Spectrogramme -> Onde)
    griffin_lim = torchaudio.transforms.GriffinLim(
        n_fft=1024, 
        n_iter=32, 
        hop_length=256,
        win_length=1024,
        power=1.0
    ).to(DEVICE)
    
    # C. Ex√©cution du Pipeline
    # 1. Le mod√®le sort des log-mels, on repasse en √©chelle normale avec exp()
    mel_spectrogram = torch.exp(spectrogram)
    
    # 2. On "d√©compresse" : Mel (80) -> Lin√©aire (513)
    linear_spectrogram = inv_mel_scale(mel_spectrogram)
    
    # 3. On reconstruit la phase et l'onde sonore
    waveform = griffin_lim(linear_spectrogram)

    # 6. Sauvegarde
    save_path = os.path.join(OUTPUT_FOLDER, "test_matcha.wav")
    torchaudio.save(save_path, waveform.cpu(), sample_rate=22050)
    print(f"‚ú® Audio sauvegard√© dans : {save_path}")

    # (Optionnel) Afficher le spectrogramme
    plt.figure(figsize=(10, 4))
    plt.imshow(spectrogram.squeeze().cpu().numpy(), origin='lower', aspect='auto')
    plt.title("Spectrogramme G√©n√©r√©")
    plt.colorbar()
    plt.savefig(os.path.join(OUTPUT_FOLDER, "spectrogram.png"))
    print("üìä Spectrogramme sauvegard√©.")

if __name__ == "__main__":
    main()