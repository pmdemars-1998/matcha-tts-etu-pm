{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b6b9b846",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Audio, display\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "\n",
    "# 1. Ajustement du chemin pour trouver le module 'matcha'\n",
    "root_path = r'D:\\Master_SAR\\MLA\\PJT\\Git\\Matcha-TTS-etu-UPMC-ENSAM'\n",
    "if root_path not in sys.path:\n",
    "    sys.path.append(root_path)\n",
    "\n",
    "# 2. Imports des modules de donn√©es\n",
    "from matcha.data_management.ljspeech_datamodule import LJSpeechDataModule\n",
    "from matcha.utils.utils import plot_spectrogram\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8da409",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset d'entra√Ænement : 12839 √©chantillons\n",
      "Dataset de validation : 261 √©chantillons\n"
     ]
    }
   ],
   "source": [
    "# Chemin vers le dossier racine de vos donn√©es LJSpeech\n",
    "data_dir = r\"D:\\Master_SAR\\MLA\\PJT\\Git\\Matcha-TTS-etu-UPMC-ENSAM\\data\\LJSpeech-1.1\"\n",
    "\n",
    "# Initialisation\n",
    "dm = LJSpeechDataModule(data_dir=data_dir, batch_size=4, num_workers=0) # num_workers=0 est plus stable pour l'exemple\n",
    "dm.setup()\n",
    "\n",
    "print(f\"Dataset d'entra√Ænement : {len(dm.train_ds)} √©chantillons\")\n",
    "print(f\"Dataset de validation : {len(dm.val_ds)} √©chantillons\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd3dbfcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forme du Batch Texte (x) : torch.Size([4, 147]) -> [Batch, Max_Len]\n",
      "Forme du Batch Mel (y)   : torch.Size([4, 80, 781]) -> [Batch, 80, Max_Time]\n",
      "\n",
      "üîç V√©rification de la phrase la plus courte (Index 0, Longueur r√©elle 46) :\n",
      "Valeurs de fin de s√©quence : [0, 0, 0, 0, 0]\n",
      "SUCC√àS : Le padding (0) est pr√©sent en fin de s√©quence.\n"
     ]
    }
   ],
   "source": [
    "# 1. R√©cup√©ration d'un Batch\n",
    "train_loader = dm.train_dataloader()  # 1. R√©cup√®re le DataLoader d'entra√Ænement\n",
    "batch = next(iter(train_loader))      # 2. Charge le premier batch // iter pour liste le dataset en it√©ration et next pour le premier de la liste\n",
    "\n",
    "\n",
    "\n",
    "x, x_lens = batch[\"x\"], batch[\"x_lengths\"]\n",
    "y, y_lens = batch[\"y\"], batch[\"y_lengths\"]\n",
    "\n",
    "print(f\"Forme du Batch Texte (x) : {x.shape} -> [Batch, Max_Len]\")\n",
    "print(f\"Forme du Batch Mel (y)   : {y.shape} -> [Batch, 80, Max_Time]\")\n",
    "\n",
    "# 2. V√©rification du padding sur le texte\n",
    "# On cherche la phrase la plus courte du batch\n",
    "shortest_idx = torch.argmin(x_lens)\n",
    "print(f\"\\nüîç V√©rification de la phrase la plus courte (Index {shortest_idx}, Longueur r√©elle {x_lens[shortest_idx]}) :\")\n",
    "print(f\"Valeurs de fin de s√©quence : {x[shortest_idx, -5:].tolist()}\")\n",
    "\n",
    "if x[shortest_idx, -1] == 0:\n",
    "    print(\"SUCC√àS : Le padding (0) est pr√©sent en fin de s√©quence.\")\n",
    "else:\n",
    "    print(\"ATTENTION : Le dernier √©l√©ment n'est pas un 0. V√©rifiez votre fonction collate.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "matcha-tts-develop",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
